{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-21T15:20:51.775803Z","iopub.execute_input":"2022-02-21T15:20:51.776379Z","iopub.status.idle":"2022-02-21T15:20:51.802109Z","shell.execute_reply.started":"2022-02-21T15:20:51.776282Z","shell.execute_reply":"2022-02-21T15:20:51.801455Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Perceptron and Boolean Functions","metadata":{}},{"cell_type":"markdown","source":"### The **Perceptron** is a linear machine learning algorithm for binary classification tasks. It is  the simplest type of neural network model.\n\n(Perceptron — это линейный алгоритм машинного обучения для задач бинарной классификации.\n\nЭто самый простой тип модели нейронной сети.)\n\n### It consists of a single node or neuron that takes a row of data as input and predicts a class label. This is achieved by calculating the weighted sum of the inputs and a bias (set to 1). The weighted sum of the input of the model is called the activation.\n\n- **Activation** = Weights * Inputs + Bias\n\n### If the activation is above 0.0, the model will output 1.0; otherwise, it will output 0.0.\n\n- **Predict 1**: If Activation > 0.0\n- **Predict 0**: If Activation <= 0.0\n\n(Он состоит из одного узла или нейрона, который принимает строку данных в качестве входных данных и предсказывает метку класса. Это достигается путем вычисления взвешенной суммы входных данных и смещения (установленного на 1). Взвешенная сумма входных данных модели называется активацией.\n\n- **Активация**\n    \n    = веса * входы + смещение\n    \n\nЕсли активация выше 0,0, модель выдаст 1,0; в противном случае будет выведено 0.0.\n\n- **Прогноз 1**\n    \n    : если активация> 0,0\n    \n- **Прогноз 0**\n    \n    : если активация <= 0,0)\n    \n    ### The Perceptron is a linear classification algorithm. This means that it learns a decision boundary that separates two classes using a line (called a hyperplane) in the feature space. As such, it is appropriate for those problems where the classes can be separated well by a line or linear model, referred to as linearly separable.\n    \n    (Персептрон — это алгоритм линейной классификации. Это означает, что он изучает границу решения, которая разделяет два класса с помощью линии (называемой гиперплоскостью) в пространстве признаков. Таким образом, он подходит для тех задач, где классы могут быть хорошо разделены линейной или линейной моделью, называемой линейно разделимой.)\n    \n   ## AND, OR, NAND and XOR.\n    \n   ### Logic gates are the most basic materials to implement digital components. The use of logic gates ranges from computer architecture to the field of electronics.\n    \n    ### These gates deal with binary values, either 0 or 1. Different types of gates take different numbers of input, but all of them provide a single output. These logic gates when combined form complicated circuits.\n    \n\n(Логические вентили — это самые основные материалы для реализации цифровых компонентов. Использование логических вентилей варьируется от компьютерной архитектуры до области электроники.\n\nЭти вентили имеют дело с двоичными значениями, либо 0, либо 1. Различные типы вентилей принимают разное количество входных данных, но все они обеспечивают один выход. Эти логические элементы в сочетании образуют сложные схемы.)","metadata":{}},{"cell_type":"markdown","source":"# Task:","metadata":{}},{"cell_type":"markdown","source":"## Create and train a Single-layer Perceptron and MLP (Multi-layered perceptron) , implement an algorithm for\n\n## classifying logical functions - AND, OR, NAND and XOR.\n\n(Задача: Создать и обучить Однослойный Персептрон и MLP ( Multi-layered perceptron) , реализовать алгоритм классификации логических функций -  AND, OR, NAND и XOR.)","metadata":{}},{"cell_type":"code","source":"# Importing Python libraries\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:29:51.415019Z","iopub.execute_input":"2022-02-21T15:29:51.415351Z","iopub.status.idle":"2022-02-21T15:29:52.468473Z","shell.execute_reply.started":"2022-02-21T15:29:51.415318Z","shell.execute_reply":"2022-02-21T15:29:52.467468Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Configuring plotting params\nsns.set_style('darkgrid')\nsns.set_context('poster')\nplt.rcParams[\"figure.figsize\"] = [20, 10]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:29:58.491442Z","iopub.execute_input":"2022-02-21T15:29:58.492077Z","iopub.status.idle":"2022-02-21T15:29:58.497124Z","shell.execute_reply.started":"2022-02-21T15:29:58.492037Z","shell.execute_reply":"2022-02-21T15:29:58.496129Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data (inputs)\n### This data is the same for each kind of logic gate, since they all take in two boolean variables as input. (Эти данные одинаковы для каждого типа логических элементов, поскольку все они принимают на вход две булевы переменные.)","metadata":{}},{"cell_type":"code","source":"data = np.array([\n                [0, 0],\n                [0, 1],\n                [1, 0],\n                [1, 1]])\nprint(data)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:30:13.963767Z","iopub.execute_input":"2022-02-21T15:30:13.964222Z","iopub.status.idle":"2022-02-21T15:30:13.972961Z","shell.execute_reply.started":"2022-02-21T15:30:13.964172Z","shell.execute_reply":"2022-02-21T15:30:13.972100Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Outputs","metadata":{}},{"cell_type":"code","source":"target_and = np.array(\n    [\n        [0],\n        [0],\n        [0],\n        [1]])\n\ntarget_or = np.array(\n    [\n        [0],\n        [1], \n        [1],\n        [1]])\n\ntarget_nand = np.array(\n    [\n        [1],\n        [1],\n        [1],\n        [0]])\n\ntarget_xor = np.array(\n    [\n        [0],\n        [1],\n        [1], \n        [0]])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:30:24.142891Z","iopub.execute_input":"2022-02-21T15:30:24.143181Z","iopub.status.idle":"2022-02-21T15:30:24.150839Z","shell.execute_reply.started":"2022-02-21T15:30:24.143149Z","shell.execute_reply":"2022-02-21T15:30:24.150196Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# The Perceptron Class\n","metadata":{}},{"cell_type":"markdown","source":"## Structure and Properties\n\n## Input Nodes\n### These nodes contain the input to the network. \n(Эти узлы содержат вход в сеть)\n\n## Weights and Biases\n### These parameters are what we update when we talk about “training” a model. They are initialized to some random value or set to 0 and updated as the training progresses. The bias is analogous to a weight independent of any input node. Basically, it makes the model more flexible, since you can “move” the activation function around.\n\n(Именно эти параметры мы обновляем, когда говорим об «обучении» модели. Они инициализируются некоторым случайным значением или устанавливаются в 0 и обновляются по мере прохождения обучения. Смещение аналогично весу, независимому от любого входного узла. По сути, это делает модель более гибкой, поскольку вы можете «перемещать» функцию активации.)\n\n## Activation Function\n### Though there are many kinds of activation functions, we’ll be using a simple linear activation function for our perceptron. The linear activation function has no effect on its input and outputs it as is.\n\n(Хотя существует много видов функций активации, мы будем использовать для нашего персептрона простую линейную функцию активации. Линейная функция активации не влияет на входные данные и выводит их как есть.)\n\n## Classification\n ### We’ll be modelling this as a classification problem, so Class 1 would represent an  value of 1, while Class 0 would represent a value of 0.\n \n (Мы будем моделировать это как проблему классификации, поэтому класс 1 будет представлять значение , равное 1, а класс 0 будет представлять значение 0.)\n\n## Training algorithm\n### Here, we cycle through the data indefinitely, keeping track of how many consecutive datapoints we correctly classified. If we manage to classify everything in one stretch, we terminate our algorithm. If not, we reset our counter, update our weights and continue the algorithm.\n\n(Здесь мы бесконечно циклически перебираем данные, отслеживая, сколько последовательных точек данных мы правильно классифицировали. Если нам удастся классифицировать все за один раз, мы завершаем наш алгоритм.\nЕсли нет, мы сбрасываем наш счетчик, обновляем наши веса и продолжаем алгоритм.)","metadata":{}},{"cell_type":"code","source":"class Perceptron:\n    \"\"\"\n    Create a perceptron.\n\n    data: A 4x2 matrix with the input data. (data: Матрица 4x2 с входными данными)\n\n    target: A 4x1 matrix with the perceptron's expected outputs  (target: матрица 4x1 с выходными данными персептрона))\n\n    lr: the learning rate. Defaults to 0.01  (скорость обучения. По умолчанию 0,01)\n\n    input_nodes: the number of nodes in the input layer of the perceptron.\n    (input_nodes: количество узлов во входном слое персептрона)\n        Should be equal to the second dimension of train_data.\n        (Должно быть равно второму измерению данных.))\n        \n    \"\"\"\n        \n    \n    def __init__(self, data, target, lr=0.01, input_nodes=2):\n            \n            \n        self.data = data\n        self.target = target\n        self.lr = lr\n        self.input_nodes = input_nodes\n        \n        # randomly initialize the weights and set the bias to -1.\n        # (случайным образом инициализируйте веса и установите смещение на -1.)\n        \n        self.w = np.random.uniform(size=self.input_nodes)\n        self.b = -1\n        \n        # node_val hold the values of each node at a given point of time.\n        # (node_val содержит значения каждого узла в данный момент времени.)\n        \n        self.node_val = np.zeros(self.input_nodes)\n        \n        # tracks how the number of consecutively correct classifications changes in each iteration\n        # (отслеживает, как количество последовательно правильных классификаций )\n        # changes in each iteration (изменения в каждой итерации)\n        \n        self.correct_iter = [0]\n        \n        \n    def _gradient(self, node, exp, output):\n        \"\"\"\n        Return the gradient for a weight. (Вернуть градиент для веса)\n        This is the value of delta-w. (Это значение delta-w)\n        \"\"\"\n        return node * (exp - output)\n    \n    \n    def update_weights(self, exp, output):\n        \"\"\"\n        Update weights and bias based on their respective gradients\n        (Обновите веса и смещения на основе их соответствующих градиентов)\n        \"\"\"\n        for i in range(self.input_nodes):\n            self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n\n        # the value of the bias node can be considered as being 1 and the weight between this node\n        # (значение узла смещения можно считать равным 1, а вес между этим узлом)\n        # and the output node being self.b  (и выходной узел self.b)\n        self.b += self.lr * self._gradient(1, exp, output)\n        \n        \n    \n    def forward(self, datapoint):\n        \"\"\"\n        One forward pass through the perceptron. (Один прямой проход через персептрон)\n        Implementation of \"wX + b\". (Реализация \"wX+b\")\n        \"\"\"\n        return self.b + np.dot(self.w, datapoint)\n    \n    \n    def classify(self, datapoint):\n        \"\"\"\n        Return the class to which a datapoint belongs based on \n        (Возвращает класс, к которому принадлежит точка данных, на основе)\n        the perceptron's output for that point. (выход персептрона для этой точки.)\n        \"\"\"\n        if self.forward(datapoint) >= 0:\n            return 1\n\n        return 0\n\n    \n    def plot(self, h=0.01):\n        \"\"\"\n        Generate plot of input data and decision boundary.\n        (Сгенерируйте график входных данных и границы решения.)\n        \"\"\"\n        # setting plot properties like size, theme and axis limits\n        # (установка свойств графика, таких как размер, тема и ограничения по осям)\n        \n        sns.set_style('darkgrid')\n        plt.figure(figsize=(10, 10))\n\n        plt.axis('scaled')\n        plt.xlim(-0.1, 1.1)\n        plt.ylim(-0.1, 1.1)\n\n        colors = {\n            0: \"ro\",\n            1: \"go\"\n        }\n\n        for i in range(len(self.data)):\n            plt.plot([self.data[i][0]],\n                     [self.data[i][1]],\n                     colors[self.target[i][0]],\n                     markersize=20)\n\n        x_range = np.arange(-0.1, 1.1, h)\n        y_range = np.arange(-0.1, 1.1, h)\n\n        # creating a mesh to plot decision boundary (создание сетки для построения границы решения)\n        \n        xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n        Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n\n        # using the contourf function to create the plot  (используя функцию контура для создания графика)\n        \n        plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n        \n        \n    def train(self):\n        \"\"\"\n        Train a single layer perceptron. ( Обучить однослойный персептрон)\n        \"\"\"\n        # the number of consecutive correct classifications\n        # (количество последовательных правильных классификаций)\n        correct_counter = 0\n        iterations = 0\n\n        for train, target in cycle(zip(self.data, self.target)):\n            # end if all points are correctly classified\n            if correct_counter == len(self.data):\n                break\n\n            # a single layer perceptron can't model the xor function\n            # (однослойный персептрон не может моделировать функцию xor)\n            # so it'll never converge!  (так что никогда не сойдется!)\n            \n            if iterations > 1000:\n                print(\"1000 iterations exceded without convergence! A single layered perceptron can't handle the XOR problem.\")\n                break\n\n            output = self.classify(train)\n            self.node_val = train\n            iterations += 1\n\n            # if correctly classified, increment correct_counter\n            if output == target:\n                correct_counter += 1\n            else:\n                # if incorrectly classified, update weights and reset correct_counter\n                # (если неправильно классифицирован, обновить веса и сбросить правильный_счетчик)\n                \n                self.update_weights(target, output)\n                correct_counter = 0\n        \n            self.correct_iter.append(correct_counter)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:33:52.701631Z","iopub.execute_input":"2022-02-21T15:33:52.701897Z","iopub.status.idle":"2022-02-21T15:33:52.726033Z","shell.execute_reply.started":"2022-02-21T15:33:52.701870Z","shell.execute_reply":"2022-02-21T15:33:52.725446Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing the Results (AND)","metadata":{}},{"cell_type":"code","source":"p_and = Perceptron(data, target_and)\np_and.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:35:53.611151Z","iopub.execute_input":"2022-02-21T15:35:53.611891Z","iopub.status.idle":"2022-02-21T15:35:53.616144Z","shell.execute_reply.started":"2022-02-21T15:35:53.611849Z","shell.execute_reply":"2022-02-21T15:35:53.614967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## AND - (0, 0, 0, 1)","metadata":{}},{"cell_type":"code","source":"p_and.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:36:12.633309Z","iopub.execute_input":"2022-02-21T15:36:12.633685Z","iopub.status.idle":"2022-02-21T15:36:12.963333Z","shell.execute_reply.started":"2022-02-21T15:36:12.633651Z","shell.execute_reply":"2022-02-21T15:36:12.962397Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"_ = plt.plot(p_and.correct_iter[:200])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:43:37.344844Z","iopub.execute_input":"2022-02-21T16:43:37.345181Z","iopub.status.idle":"2022-02-21T16:43:37.795392Z","shell.execute_reply.started":"2022-02-21T16:43:37.345147Z","shell.execute_reply":"2022-02-21T16:43:37.794507Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def AND(x1, x2):\n    \n    x = [x1, x2]\n    p_and = Perceptron(data, target_and)\n    p_and.train()\n\n    return p_and.classify(x)\n\n# Test AND\nAND(1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:36:32.672346Z","iopub.execute_input":"2022-02-21T15:36:32.672609Z","iopub.status.idle":"2022-02-21T15:36:32.684012Z","shell.execute_reply.started":"2022-02-21T15:36:32.672582Z","shell.execute_reply":"2022-02-21T15:36:32.683308Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Test AND\nAND(0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:36:41.302044Z","iopub.execute_input":"2022-02-21T15:36:41.302929Z","iopub.status.idle":"2022-02-21T15:36:41.311022Z","shell.execute_reply.started":"2022-02-21T15:36:41.302875Z","shell.execute_reply":"2022-02-21T15:36:41.310031Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## The single-layer perceptron correctly classified the entire data set for the logical function: AND\n\n## Since the function is linearly separable\n\n( Однослойный персептрон правильно классифицировал весь набор данных для логической функции: AND.т. к. эта функция линейно разделима.)","metadata":{}},{"cell_type":"markdown","source":"# Analyzing the Results (OR) ","metadata":{}},{"cell_type":"code","source":"p_or = Perceptron(data, target_or)\np_or.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:50:58.332851Z","iopub.execute_input":"2022-02-21T15:50:58.333630Z","iopub.status.idle":"2022-02-21T15:50:58.342170Z","shell.execute_reply.started":"2022-02-21T15:50:58.333590Z","shell.execute_reply":"2022-02-21T15:50:58.341587Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### This converges, since the data for the OR function is linearly separable. If we plot the number of correctly classified consecutive datapoints, we get the below plot. It’s  hits the value 4, meaning that it classified the entire dataset correctly.\n\n(Это сходится, поскольку данные для функции ИЛИ линейно разделимы. Если мы нанесем на график количество правильно классифицированных последовательных точек данных, мы получим приведенный ниже график. Понятно, что он достигает значения 4, что означает, что он правильно классифицировал весь набор данных.)","metadata":{}},{"cell_type":"code","source":"_ = plt.plot(p_or.correct_iter[:200])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:51:38.665493Z","iopub.execute_input":"2022-02-21T15:51:38.665940Z","iopub.status.idle":"2022-02-21T15:51:38.983134Z","shell.execute_reply.started":"2022-02-21T15:51:38.665893Z","shell.execute_reply":"2022-02-21T15:51:38.982310Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"##  OR -(0, 1, 1, 1)","metadata":{}},{"cell_type":"code","source":"p_or.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:55:21.057095Z","iopub.execute_input":"2022-02-21T15:55:21.058066Z","iopub.status.idle":"2022-02-21T15:55:21.434686Z","shell.execute_reply.started":"2022-02-21T15:55:21.057995Z","shell.execute_reply":"2022-02-21T15:55:21.433782Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def OR(x1, x2):\n    \n    x = [x1, x2]\n    p_or = Perceptron(data, target_or)\n    p_or.train()\n\n    return p_or.classify(x)\n\n# Test OR\nOR(1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:56:55.777971Z","iopub.execute_input":"2022-02-21T15:56:55.778259Z","iopub.status.idle":"2022-02-21T15:56:55.787977Z","shell.execute_reply.started":"2022-02-21T15:56:55.778215Z","shell.execute_reply":"2022-02-21T15:56:55.787342Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Test OR\nOR(1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:57:00.352845Z","iopub.execute_input":"2022-02-21T15:57:00.353456Z","iopub.status.idle":"2022-02-21T15:57:00.364396Z","shell.execute_reply.started":"2022-02-21T15:57:00.353423Z","shell.execute_reply":"2022-02-21T15:57:00.363814Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## The single-layer perceptron correctly classified the entire data set for the logical function: OR\n## Since the function is linearly separable\n( Однослойный персептрон правильно классифицировал весь набор данных для логической функции: OR.т. к. эта функция линейно разделима.)","metadata":{}},{"cell_type":"markdown","source":"# Analyzing the Results (NAND)\n","metadata":{}},{"cell_type":"code","source":"p_nand = Perceptron(data, target_nand)\np_nand.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:01:47.986063Z","iopub.execute_input":"2022-02-21T16:01:47.986352Z","iopub.status.idle":"2022-02-21T16:01:48.016332Z","shell.execute_reply.started":"2022-02-21T16:01:47.986321Z","shell.execute_reply":"2022-02-21T16:01:48.015382Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"_ = plt.plot(p_nand.correct_iter[:1000])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:02:30.939537Z","iopub.execute_input":"2022-02-21T16:02:30.939829Z","iopub.status.idle":"2022-02-21T16:02:31.234007Z","shell.execute_reply.started":"2022-02-21T16:02:30.939796Z","shell.execute_reply":"2022-02-21T16:02:31.233171Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## NAND - (1, 1, 1, 0)\nThe plot shows perfect convergence. (График показывает идеальную сходимость.)","metadata":{}},{"cell_type":"code","source":"_ = p_nand.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:03:55.110972Z","iopub.execute_input":"2022-02-21T16:03:55.111241Z","iopub.status.idle":"2022-02-21T16:03:55.487203Z","shell.execute_reply.started":"2022-02-21T16:03:55.111213Z","shell.execute_reply":"2022-02-21T16:03:55.486433Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def NAND(x1, x2):\n    \n    x = [x1, x2]\n    p_nand = Perceptron(data, target_nand)\n    p_nand.train()\n\n    return p_nand.classify(x)\n\n# Test NAND\nNAND(1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:04:31.414870Z","iopub.execute_input":"2022-02-21T16:04:31.415133Z","iopub.status.idle":"2022-02-21T16:04:31.440741Z","shell.execute_reply.started":"2022-02-21T16:04:31.415104Z","shell.execute_reply":"2022-02-21T16:04:31.439832Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Test NAND\nNAND(0, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:04:35.974468Z","iopub.execute_input":"2022-02-21T16:04:35.974734Z","iopub.status.idle":"2022-02-21T16:04:35.997496Z","shell.execute_reply.started":"2022-02-21T16:04:35.974707Z","shell.execute_reply":"2022-02-21T16:04:35.996704Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"##  The single-layer perceptron correctly classified the entire data set for the logical functions: \n\n## AND, OR, and NAND.\n\n## Since these functions are linearly separable \n\n( Однослойный персептрон правильно классифицировал весь набор данных для логических функций: AND, OR  и  NAND.\n\nт. к. эти функции линейно разделимы.)","metadata":{}},{"cell_type":"markdown","source":"# Analyzing the Results (XOR)\n## A single-layered perceptron isn't enough to model the 2d XOR function, so training will never converge. \n\n## Here, convergence means correctly classifying all training examples consecutively.\n\n(Однослойного персептрона недостаточно для моделирования функции 2d XOR, поэтому обучение никогда не сойдется. Здесь конвергенция означает правильную последовательную классификацию всех обучающих примеров.)\n","metadata":{}},{"cell_type":"code","source":"p_xor = Perceptron(data, target_xor)\np_xor.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:12:53.102274Z","iopub.execute_input":"2022-02-21T16:12:53.102538Z","iopub.status.idle":"2022-02-21T16:12:53.142197Z","shell.execute_reply.started":"2022-02-21T16:12:53.102509Z","shell.execute_reply":"2022-02-21T16:12:53.141321Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### The training loop never terminates, since a perceptron can only converge on linearly separable data. Linearly separable data basically means that you can separate data with a point in 1D, a line in 2D, a plane in 3D and so on.\n\n( Цикл обучения никогда не заканчивается, поскольку персептрон может сходиться только на линейно разделимых данных. Линейно разделяемые данные в основном означают, что вы можете разделить данные с помощью точки в 1D, линии в 2D, плоскости в 3D и так далее.)\n\n### A perceptron can only converge on linearly separable data. Therefore, it isn’t capable of imitating the XOR function.\n\n(Персептрон может сходиться только на линейно разделимых данных. Следовательно, он не способен имитировать функцию XOR.)\n\n### A perceptron must correctly classify the entire training data in one go. If we keep track of how many points it correctly classified consecutively, we get something like this.\n\n(персептрон должен правильно классифицировать все обучающие данные за один раз. Если мы отследим, сколько точек правильно классифицировано последовательно, мы получим что-то вроде этого.)","metadata":{}},{"cell_type":"code","source":"_ = plt.plot(p_xor.correct_iter[:200])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:13:42.118532Z","iopub.execute_input":"2022-02-21T16:13:42.118797Z","iopub.status.idle":"2022-02-21T16:13:42.445016Z","shell.execute_reply.started":"2022-02-21T16:13:42.118769Z","shell.execute_reply":"2022-02-21T16:13:42.444210Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### The algorithm only terminates when correct_counter hits 4 — which is the size of the training set — so this will go on indefinitely.\n(Алгоритм завершается только тогда, когда correct_counter достигает 4 — это размер тренировочного набора — так что это будет продолжаться бесконечно.)","metadata":{}},{"cell_type":"markdown","source":"## A single layer perceptron is not enough to model the 2d XOR function, so the learning cycle never ends. this function is not linearly @\n\n## separable. \n## But the XOR function can be modeled by combining AND, OR and NAND single layer perceptrons\n\n(Однослойного персептрона недостаточно для моделирования функции 2d XOR, поэтому  цикл обучения никогда не заканчивается, т.к. эта функция линейно не разделима. Но функцию XOR можно моделировать путем объединения однослойных персептронов AND, OR и NAND.)","metadata":{}},{"cell_type":"markdown","source":"# Modeling the XOR function by combining single-layered perceptrons \n( Моделирование функции XOR путем объединения однослойных персептронов)","metadata":{}},{"cell_type":"markdown","source":"### The XOR function is basically a combination of an AND, OR and NAND. gate.\n(Функция XOR в основном представляет собой комбинацию AND, OR и NAND.    \n### XOR(x1, x2) = AND(OR(x1, x2), NAND(x1, x2))","metadata":{}},{"cell_type":"code","source":"def XOR(x1, x2):\n    \"\"\"\n    Return the boolean XOR of x1 and x2 by combining individual single-layered perceptrons\n    (Верните логическое XOR x1 и x2, объединив отдельные однослойные персептроны.)\n    \"\"\"\n\n    x = [x1, x2]\n    p_or = Perceptron(data, target_or)\n    p_nand = Perceptron(data, target_nand)\n    p_and = Perceptron(data, target_and)\n\n    p_or.train()\n    p_nand.train()\n    p_and.train()\n\n    return p_and.classify([p_or.classify(x),\n                          p_nand.classify(x)])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:18:08.777066Z","iopub.execute_input":"2022-02-21T16:18:08.777861Z","iopub.status.idle":"2022-02-21T16:18:08.784140Z","shell.execute_reply.started":"2022-02-21T16:18:08.777816Z","shell.execute_reply":"2022-02-21T16:18:08.783522Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Test. \n# When we try and find the XOR of 1 and 1, we get 0 just like we expected.\n# Когда мы пытаемся найти XOR 1 и 1, мы получаем 0, как и ожидали.\n\nXOR(1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:18:19.480236Z","iopub.execute_input":"2022-02-21T16:18:19.480523Z","iopub.status.idle":"2022-02-21T16:18:19.501297Z","shell.execute_reply.started":"2022-02-21T16:18:19.480494Z","shell.execute_reply":"2022-02-21T16:18:19.500519Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Test. \nXOR(0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:18:23.186071Z","iopub.execute_input":"2022-02-21T16:18:23.186574Z","iopub.status.idle":"2022-02-21T16:18:23.211829Z","shell.execute_reply.started":"2022-02-21T16:18:23.186534Z","shell.execute_reply":"2022-02-21T16:18:23.210953Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Test.\nXOR(0, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:18:26.175115Z","iopub.execute_input":"2022-02-21T16:18:26.175393Z","iopub.status.idle":"2022-02-21T16:18:26.214086Z","shell.execute_reply.started":"2022-02-21T16:18:26.175364Z","shell.execute_reply":"2022-02-21T16:18:26.213388Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## This model correctly classified the entire data set for the logical function: XOR.\n(Такая модель правильно классифицировала весь набор данных для логическoй функции: XOR.)","metadata":{}},{"cell_type":"markdown","source":"# MLP ( Multi-layered perceptron)\n\n### A multi-layered perceptron can have hidden layers. \n(Многослойный персептрон может иметь скрытые слои.)\n\n### There are no fixed rules on the number of hidden layers or the number of nodes in each layer of a network. The best performing models are obtained through trial and error.\n\n(Не существует фиксированных правил по количеству скрытых слоев или количеству узлов в каждом слое сети. Лучшие модели получаются методом проб и ошибок.)\n\n### Let’s go with a single hidden layer with two nodes in it. We’ll be using the sigmoid function in each of our hidden layer nodes and of course, our output node.\n\n(Давайте возьмем один скрытый слой с двумя узлами в нем. Мы будем использовать сигмовидную функцию в каждом из наших узлов скрытого слоя и, конечно же, в нашем выходном узле.)","metadata":{}},{"cell_type":"markdown","source":"## The MLP Class\n\n### This class houses each component of our MLP, from training and the forward pass, to classifying and plotting the decision boundary. \n\n(Этот класс содержит каждый компонент нашего MLP, от обучения и прямого прохода до классификации и построения границы решения.)","metadata":{}},{"cell_type":"code","source":"class MLP:\n    \"\"\"\n    Create a multi-layer perceptron.\n\n    data: A 4x2 matrix with the input data.\n\n    target: A 4x1 matrix with expected outputs\n\n    lr: the learning rate. Defaults to 0.1\n\n    num_epochs: the number of times the training data goes through the model\n        while training\n    (num_epochs: сколько раз обучающие данные проходят через модель\n         во время тренировки)\n\n    num_input: the number of nodes in the input layer of the MLP.\n        Should be equal to the second dimension of data.\n    (num_input: количество узлов во входном слое MLP.\n         Должен быть равен второму измерению data.)\n         \n    num_hidden: the number of nodes in the hidden layer of the MLP.\n    (num_hidden: количество узлов в скрытом слое MLP.)\n\n    num_output: the number of nodes in the output layer of the MLP.\n        Should be equal to the second dimension of target.\n     (num_output: количество узлов в выходном слое MLP.\n         Должен быть равен второму измерению цели.)   \n    \n    \"\"\"\n    \n    \n    def __init__(self, data, target, lr=0.1, num_epochs=100, num_input=2, num_hidden=2, num_output=1):\n        self.data = data\n        self.target = target\n        self.lr = lr\n        self.num_epochs = num_epochs\n\n        # initialize both sets of weights and biases randomly\n        # (инициализировать оба набора весов и смещений случайным образом)\n        \n            # - weights_01: weights between input and hidden layer\n            # (weights_01: веса между входным и скрытым слоями)\n            \n            # - weights_12: weights between hidden and output layer\n            # (weights_12: веса между скрытым и выходным слоями)\n            \n        self.weights_01 = np.random.uniform(size=(num_input, num_hidden))\n        self.weights_12 = np.random.uniform(size=(num_hidden, num_output))\n\n        # - b01: biases for the  hidden layer\n        # (b01: смещения для скрытого слоя)\n        \n        # - b12: bias for the output layer\n        # (# - b12: смещение выходного слоя)\n        \n        self.b01 = np.random.uniform(size=(1,num_hidden))\n        self.b12 = np.random.uniform(size=(1,num_output))\n\n        self.losses = []\n\n    def update_weights(self):\n        \n        # Calculate the squared error\n        # (Вычислить квадрат ошибки)\n        \n        loss = 0.5 * (self.target - self.output_final) ** 2\n        self.losses.append(np.sum(loss))\n\n        error_term = (self.target - self.output_final)\n\n        # the gradient for the hidden layer weights\n        # (градиент веса скрытого слоя)\n        \n        grad01 = self.data.T @ (((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) *\n                                      self._delsigmoid(self.hidden_out))\n\n        # the gradient for the output layer weights\n        # (градиент для весов выходного слоя)\n        \n        grad12 = self.hidden_out.T @ (error_term * self._delsigmoid(self.output_final))\n\n        # updating the weights by the learning rate times their gradient\n        # обновление весов по скорости обучения, умноженной на их градиент\n        \n        self.weights_01 += self.lr * grad01\n        self.weights_12 += self.lr * grad12\n\n        # update the biases the same way\n        # (обновите biases таким же образом)\n        \n        self.b01 += np.sum(self.lr * ((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) *\n                           self._delsigmoid(self.hidden_out), axis=0)\n        self.b12 += np.sum(self.lr * error_term * self._delsigmoid(self.output_final), axis=0)\n\n    def _sigmoid(self, x):\n        \"\"\"\n        The sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def _delsigmoid(self, x):\n        \"\"\"\n        The first derivative of the sigmoid function wrt x\n        (Первая производная сигмовидной функции)\n        \"\"\"\n        return x * (1 - x)\n\n    def forward(self, batch):\n        \"\"\"\n        A single forward pass through the network. (Один прямой проход по сети.)\n        Implementation of wX + b  (Реализация wX+b)\n        \"\"\"\n\n        self.hidden_ = np.dot(batch, self.weights_01) + self.b01\n        self.hidden_out = self._sigmoid(self.hidden_)\n\n        self.output_ = np.dot(self.hidden_out, self.weights_12) + self.b12\n        self.output_final = self._sigmoid(self.output_)\n\n        return self.output_final\n\n    def classify(self, datapoint):\n        \"\"\"\n        Return the class to which a datapoint belongs based on\n        the perceptron's output for that point.\n        (Возвращает класс, к которому принадлежит точка данных, на основе\n         выхода персептрона для этой точки.)\n        \"\"\"\n        datapoint = np.transpose(datapoint)\n        if self.forward(datapoint) >= 0.5:\n            return 1\n\n        return 0\n\n    def plot(self, h=0.01):\n        \"\"\"\n        Generate plot of input data and decision boundary.\n        (Сгенерируйте график входных данных и границы решения.)\n        \"\"\"\n        # setting plot properties like size, theme and axis limits\n        sns.set_style('darkgrid')\n        plt.figure(figsize=(10, 10))\n\n        plt.axis('scaled')\n        plt.xlim(-0.1, 1.1)\n        plt.ylim(-0.1, 1.1)\n\n        colors = {\n            0: \"ro\",\n            1: \"go\"\n        }\n\n        # plotting the four datapoints\n        for i in range(len(self.data)):\n            plt.plot([self.data[i][0]],\n                     [self.data[i][1]],\n                     colors[self.target[i][0]],\n                     markersize=20)\n\n        x_range = np.arange(-0.1, 1.1, h)\n        y_range = np.arange(-0.1, 1.1, h)\n\n        # creating a mesh to plot decision boundary\n        xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n        Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n\n        # using the contourf function to create the plot\n        plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n\n    def train(self):\n        \"\"\"\n        Train an MLP. Runs through the data num_epochs number of times.\n        (Обучение МЛП. Проходит через данные num_epochs количество раз.)\n        A forward pass is done first, followed by a backward pass (backpropagation)\n        (Сначала выполняется прямой проход, а затем обратный проход (обратное распространение).)\n        where the networks parameter's are updated.\n        (где параметры сети обновляются)\n        \"\"\"\n        for epoch in range(self.num_epochs):\n\n            self.forward(self.data)\n            self.update_weights()\n            \n            if epoch % 500 == 0:\n                print(\"Loss: \", self.losses[epoch]) ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:23:09.682408Z","iopub.execute_input":"2022-02-21T16:23:09.682676Z","iopub.status.idle":"2022-02-21T16:23:09.711033Z","shell.execute_reply.started":"2022-02-21T16:23:09.682648Z","shell.execute_reply":"2022-02-21T16:23:09.710430Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing the Results (XOR)\n\n\n## Let’s train our MLP with a learning rate of 0.25 over 9000 epochs. \n(Давайте обучим наш MLP со скоростью обучения 0,25 за 9000 эпох.)","metadata":{}},{"cell_type":"code","source":"mlp = MLP(data, target_xor, 0.25, 9000)\nmlp.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:24:15.939085Z","iopub.execute_input":"2022-02-21T16:24:15.939608Z","iopub.status.idle":"2022-02-21T16:24:16.856624Z","shell.execute_reply.started":"2022-02-21T16:24:15.939566Z","shell.execute_reply":"2022-02-21T16:24:16.855708Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## If we plot the values of our loss function, we get the following plot after about 9000 iterations, showing that our model has indeed converged.\n\n(Если мы построим значения нашей функции потерь, мы получим следующий график примерно после 9000 итераций, показывающий, что наша модель действительно сошлась.)","metadata":{}},{"cell_type":"code","source":"_ = plt.plot(mlp.losses)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:27:34.446871Z","iopub.execute_input":"2022-02-21T16:27:34.447369Z","iopub.status.idle":"2022-02-21T16:27:34.692868Z","shell.execute_reply.started":"2022-02-21T16:27:34.447336Z","shell.execute_reply":"2022-02-21T16:27:34.691909Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## A clear non-linear decision boundary is created here with our generalized neural network, or MLP.\n (Четкая нелинейная граница решения создается здесь с помощью нашей обобщенной нейронной сети или MLP.)","metadata":{}},{"cell_type":"markdown","source":"## XOR - (0, 1, 1, 0)","metadata":{}},{"cell_type":"code","source":"mlp.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:31:03.430666Z","iopub.execute_input":"2022-02-21T16:31:03.431577Z","iopub.status.idle":"2022-02-21T16:31:04.089359Z","shell.execute_reply.started":"2022-02-21T16:31:03.431538Z","shell.execute_reply":"2022-02-21T16:31:04.088388Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## MLP ( Multi-layered perceptron) correctly classified the entire data set for the logical function: XOR.\n\n( MLP ( Multi-layered perceptron) правильно классифицировал весь набор данных для логическoй функции: XOR.)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}